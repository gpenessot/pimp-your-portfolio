# ğŸ¯ PrÃ©diction de Churn Client - TÃ©lÃ©coms

> Identifier les clients Ã  risque 3 mois avant leur dÃ©part pour lancer des actions de rÃ©tention.

## ğŸ“Š ProblÃ©matique Business

**Contexte** : L'entreprise de tÃ©lÃ©communications perd **25% de ses clients chaque annÃ©e**.

**CoÃ»t** : Le coÃ»t d'acquisition d'un nouveau client (CAC) est **5x plus Ã©levÃ©** que la rÃ©tention.

**Objectif** : Identifier les clients Ã  risque **3 mois avant** leur dÃ©part pour lancer des campagnes de rÃ©tention ciblÃ©es.

**Impact attendu** :
- RÃ©duction du churn de 25% â†’ 18% (-28%)
- Ã‰conomie annuelle : 2Mâ‚¬ (10,000 clients Ã— 200â‚¬/client)
- ROI campagne rÃ©tention : 5:1

---

## ğŸ—‚ï¸ Dataset

**Source** : [Telco Customer Churn - IBM](https://www.kaggle.com/datasets/blastchar/telco-customer-churn)

**Volume** : 7,043 clients avec 21 features

**Features clÃ©s** :
- DÃ©mographie : `SeniorCitizen`, `Partner`, `Dependents`
- Services : `PhoneService`, `InternetService`, `StreamingTV`
- Contrat : `Contract`, `MonthlyCharges`, `TotalCharges`, `tenure`
- Support : `TechSupport`, `OnlineBackup`
- Target : `Churn` (Yes/No)

**TÃ©lÃ©chargement** :
```bash
# Option 1 : Kaggle CLI (nÃ©cessite API key)
kaggle datasets download -d blastchar/telco-customer-churn

# Option 2 : TÃ©lÃ©chargement manuel
# https://www.kaggle.com/datasets/blastchar/telco-customer-churn
# Placer le CSV dans data/raw/
```

---

## ğŸ—ï¸ Architecture Technique

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Data Source  â”‚â”€â”€â”€â–¶â”‚ Feature Eng.    â”‚â”€â”€â”€â–¶â”‚ ML Pipelineâ”‚â”€â”€â”€â–¶â”‚   API   â”‚â”€â”€â”€â–¶â”‚ Dashboard â”‚
â”‚  (Kaggle)    â”‚    â”‚   (pandas)      â”‚    â”‚ (sklearn)  â”‚    â”‚(FastAPI)â”‚    â”‚(Streamlit)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    CSV                 numpy                 XGBoost          Docker         Plotly
```

---

## ğŸ“¦ Structure du Projet

```
01-churn-prediction-telecom/
â”œâ”€â”€ README.md                     # Ce fichier
â”œâ”€â”€ requirements.txt              # DÃ©pendances Python
â”œâ”€â”€ .gitignore                    # Fichiers Ã  ignorer
â”œâ”€â”€ Dockerfile                    # Container API
â”œâ”€â”€ docker-compose.yml            # Orchestration
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                      # DonnÃ©es brutes (non versionnÃ©es)
â”‚   â”œâ”€â”€ processed/                # DonnÃ©es nettoyÃ©es
â”‚   â””â”€â”€ README.md                 # Instructions tÃ©lÃ©chargement
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_eda.ipynb              # Analyse exploratoire
â”‚   â”œâ”€â”€ 02_feature_engineering.ipynb
â”‚   â””â”€â”€ 03_modeling.ipynb         # EntraÃ®nement modÃ¨le
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py                 # Configuration
â”‚   â”œâ”€â”€ data_loader.py            # Chargement donnÃ©es
â”‚   â”œâ”€â”€ feature_engineering.py    # Features custom
â”‚   â”œâ”€â”€ model_trainer.py          # EntraÃ®nement
â”‚   â””â”€â”€ predictor.py              # InfÃ©rence
â”‚
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ main.py                   # FastAPI app
â”‚   â”œâ”€â”€ schemas.py                # Pydantic models
â”‚   â””â”€â”€ routers/
â”‚       â”œâ”€â”€ predict.py            # Endpoint /predict
â”‚       â””â”€â”€ health.py             # Endpoint /health
â”‚
â”œâ”€â”€ dashboards/
â”‚   â””â”€â”€ streamlit_app.py          # Dashboard interactif
â”‚
â”œâ”€â”€ models/
â”‚   â””â”€â”€ .gitkeep                  # ModÃ¨les sauvegardÃ©s (MLflow)
â”‚
â””â”€â”€ tests/
    â”œâ”€â”€ test_feature_engineering.py
    â””â”€â”€ test_api.py
```

---

## ğŸš€ Quick Start

### 1. Installation

```bash
# Cloner le template
git clone <repo-url>
cd 01-churn-prediction-telecom

# CrÃ©er environnement virtuel
python -m venv venv
source venv/bin/activate  # Linux/Mac
# venv\Scripts\activate   # Windows

# Installer dÃ©pendances
pip install -r requirements.txt
```

### 2. TÃ©lÃ©charger les donnÃ©es

```bash
# Suivre les instructions dans data/README.md
# Placer telco_churn.csv dans data/raw/
```

### 3. Analyse exploratoire

```bash
jupyter notebook notebooks/01_eda.ipynb
```

### 4. EntraÃ®ner le modÃ¨le

```bash
python src/model_trainer.py
# Le modÃ¨le sera sauvegardÃ© dans models/
```

### 5. Lancer l'API

```bash
# Option 1 : Local
uvicorn api.main:app --reload --port 8000

# Option 2 : Docker
docker-compose up --build
```

### 6. Tester l'API

```bash
curl -X POST http://localhost:8000/predict \
  -H "Content-Type: application/json" \
  -d '{
    "tenure": 12,
    "MonthlyCharges": 70.5,
    "TotalCharges": 846.0,
    "Contract": "Month-to-month",
    "InternetService": "Fiber optic"
  }'
```

### 7. Lancer le dashboard

```bash
streamlit run dashboards/streamlit_app.py
```

---

## ğŸ¯ CritÃ¨res de RÃ©ussite

| MÃ©trique | Objectif | Justification |
|----------|----------|---------------|
| **Recall Churn** | > 80% | Minimiser clients perdus non dÃ©tectÃ©s (faux nÃ©gatifs) |
| **Precision** | > 70% | Limiter fausses alertes (coÃ»t campagnes inutiles) |
| **API Latence** | < 100ms | Temps rÃ©el pour intÃ©gration CRM |
| **F1-Score** | > 0.75 | Ã‰quilibre recall/precision |

---

## ğŸ› ï¸ Stack Technique

**Machine Learning** :
- Python 3.10+
- pandas, numpy
- scikit-learn 1.3+
- XGBoost 2.0+
- SHAP (explainability)

**MLOps** :
- MLflow (tracking, registry)
- Docker (containerisation)

**API** :
- FastAPI 0.100+
- uvicorn (server ASGI)
- pydantic (validation)

**Visualisation** :
- Streamlit 1.28+
- plotly
- seaborn, matplotlib

**Dev Tools** :
- Jupyter Notebook
- pytest (tests)
- black (formatting)
- Git + GitHub

---

## ğŸ’¡ Features Engineering ClÃ©s

**Ratios** :
```python
# Ratio prix/anciennetÃ© (indicateur augmentation)
df['price_increase_ratio'] = df['MonthlyCharges'] / (df['tenure'] + 1)

# Engagement services (nombre de services souscrits)
service_cols = ['PhoneService', 'InternetService', 'OnlineBackup', 'StreamingTV']
df['service_engagement'] = df[service_cols].apply(lambda x: x.str.contains('Yes').sum(), axis=1)
```

**DÃ©lais** :
```python
# DÃ©lai depuis dernier contact support (simulation)
df['days_since_support'] = np.random.randint(0, 365, len(df))
```

**Segmentation** :
```python
# Segmentation anciennetÃ©
df['tenure_segment'] = pd.cut(df['tenure'], bins=[0, 12, 24, 60, 100],
                               labels=['<1yr', '1-2yr', '2-5yr', '>5yr'])
```

---

## ğŸ”§ Gestion du DÃ©sÃ©quilibre

Le dataset a un dÃ©sÃ©quilibre (26.5% churn vs 73.5% non-churn).

**Techniques** :
```python
# Option 1 : Class weight
from sklearn.linear_model import LogisticRegression
model = LogisticRegression(class_weight='balanced')

# Option 2 : SMOTE (oversampling)
from imblearn.over_sampling import SMOTE
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)

# Option 3 : Focal Loss (pour deep learning)
# Penalise davantage les erreurs sur classe minoritaire
```

---

## ğŸ“Š MÃ©triques Business

**Calcul ROI campagne rÃ©tention** :
```python
# HypothÃ¨ses
n_clients_at_risk = 1000
true_churn_rate = 0.80  # Parmi ceux prÃ©dits
retention_campaign_success = 0.30  # 30% retenus
customer_ltv = 200  # Lifetime value moyenne
campaign_cost_per_client = 20

# Calcul
clients_saved = n_clients_at_risk * true_churn_rate * retention_campaign_success
revenue_saved = clients_saved * customer_ltv
campaign_cost = n_clients_at_risk * campaign_cost_per_client
roi = (revenue_saved - campaign_cost) / campaign_cost

print(f"Clients sauvÃ©s : {clients_saved}")
print(f"Revenu sauvÃ© : {revenue_saved}â‚¬")
print(f"ROI : {roi:.1%}")  # 5:1 attendu
```

---

## ğŸš¢ DÃ©ploiement

### Docker

```bash
# Build image
docker build -t churn-api .

# Run container
docker run -p 8000:8000 churn-api
```

### Streamlit Cloud

```bash
# 1. Push sur GitHub
git add .
git commit -m "feat: churn prediction app"
git push origin main

# 2. DÃ©ployer sur https://streamlit.io/cloud
# Connecter repo GitHub
# SÃ©lectionner dashboards/streamlit_app.py
```

---

## ğŸ“š Ressources

**Datasets** :
- [Telco Customer Churn - Kaggle](https://www.kaggle.com/datasets/blastchar/telco-customer-churn)

**Articles** :
- [Survival Analysis for Churn Prediction](https://towardsdatascience.com/survival-analysis-churn-prediction)
- [SHAP for Model Explainability](https://shap.readthedocs.io/)

**Benchmarks** :
- State-of-the-art : F1 = 0.82 (XGBoost + feature engineering)
- Baseline : F1 = 0.65 (Logistic Regression)

---

## ğŸ“ TODO

- [ ] TÃ©lÃ©charger dataset Kaggle
- [ ] ComplÃ©ter notebooks EDA
- [ ] ImplÃ©menter feature engineering
- [ ] EntraÃ®ner modÃ¨le XGBoost
- [ ] Atteindre Recall > 80%
- [ ] CrÃ©er API FastAPI
- [ ] Tester latence < 100ms
- [ ] Dashboard Streamlit
- [ ] SHAP values pour explainability
- [ ] Dockeriser l'application
- [ ] DÃ©ployer sur Streamlit Cloud
- [ ] Ã‰crire README final avec rÃ©sultats

---

## ğŸ‘¤ Auteur

**Votre Nom**
- GitHub : [@votre-username](https://github.com/votre-username)
- LinkedIn : [Votre Profil](https://linkedin.com/in/votre-profil)
- Portfolio : [votre-site.com](https://votre-site.com)

---

## ğŸ“„ Licence

MIT License - Libre d'utilisation pour votre portfolio.
